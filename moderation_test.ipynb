{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T15:25:12.842705Z",
     "start_time": "2025-09-10T15:25:12.836094Z"
    }
   },
   "source": [
    "import os, json, base64, glob\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "import concurrent.futures"
   ],
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T18:35:29.917845Z",
     "start_time": "2025-09-09T18:35:29.596219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_VISION_AGENT = \"gpt-4o\"  # your taxonomy-based agent (from earlier)\n",
    "MODEL_MODERATION = \"omni-moderation-latest\"  # OpenAI multimodal moderation\n",
    "\n",
    "with open(\"credentials.txt\", \"rb\") as f:\n",
    "    open_ai_key = f.readline().decode('ascii')\n",
    "\n",
    "client = OpenAI(api_key=open_ai_key)"
   ],
   "id": "5f7d586e511fe8bf",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T18:51:10.549989Z",
     "start_time": "2025-09-09T18:51:10.543427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def b64_image(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return \"data:image/\" + Path(path).suffix[1:].lower() + \";base64,\" + base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def openai_moderate_image(img_b64, optional_text=None):\n",
    "    \"\"\"\n",
    "    Send image (+ optional text) to OpenAI Moderation.\n",
    "    Returns a dict of categories/scores/flags.\n",
    "    \"\"\"\n",
    "    # The Moderation API accepts text and image content; payload styles may evolve.\n",
    "    resp = client.moderations.create(\n",
    "        model=MODEL_MODERATION,\n",
    "        input=[\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": img_b64}},\n",
    "            *([{\"type\": \"text\", \"text\": optional_text}] if optional_text else [])\n",
    "        ]\n",
    "    )\n",
    "    return resp"
   ],
   "id": "f689cb81c310852b",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T21:31:41.478413Z",
     "start_time": "2025-09-10T21:31:41.469744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image_paths = glob.glob(\"C:\\\\Users\\\\benja\\\\Desktop\\\\prompt_updates\\\\images\\\\*.png\")\n",
    "# image_paths = [p for p in image_paths if \"jesus\" in p or \"clown\" in p or \"drinking\" in p or \"sexy\" in p]\n",
    "image_paths"
   ],
   "id": "ff60221736bf76e1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\benja\\\\Desktop\\\\prompt_updates\\\\images\\\\call_me_naked_alien.png',\n",
       " 'C:\\\\Users\\\\benja\\\\Desktop\\\\prompt_updates\\\\images\\\\captain_beer_mcdonalds.png',\n",
       " 'C:\\\\Users\\\\benja\\\\Desktop\\\\prompt_updates\\\\images\\\\cultural_appropriation_clown.png',\n",
       " 'C:\\\\Users\\\\benja\\\\Desktop\\\\prompt_updates\\\\images\\\\cultural_appropriation_clown_2.png',\n",
       " 'C:\\\\Users\\\\benja\\\\Desktop\\\\prompt_updates\\\\images\\\\disney_underwear.png',\n",
       " 'C:\\\\Users\\\\benja\\\\Desktop\\\\prompt_updates\\\\images\\\\drinking_godess.png',\n",
       " 'C:\\\\Users\\\\benja\\\\Desktop\\\\prompt_updates\\\\images\\\\ducky_jesus.png',\n",
       " 'C:\\\\Users\\\\benja\\\\Desktop\\\\prompt_updates\\\\images\\\\dumpster.png',\n",
       " 'C:\\\\Users\\\\benja\\\\Desktop\\\\prompt_updates\\\\images\\\\elon.png',\n",
       " 'C:\\\\Users\\\\benja\\\\Desktop\\\\prompt_updates\\\\images\\\\fat_losers.png',\n",
       " 'C:\\\\Users\\\\benja\\\\Desktop\\\\prompt_updates\\\\images\\\\fight_night.png',\n",
       " 'C:\\\\Users\\\\benja\\\\Desktop\\\\prompt_updates\\\\images\\\\hail_satan.png',\n",
       " 'C:\\\\Users\\\\benja\\\\Desktop\\\\prompt_updates\\\\images\\\\hate_shield.png',\n",
       " 'C:\\\\Users\\\\benja\\\\Desktop\\\\prompt_updates\\\\images\\\\hate_shield_2.png',\n",
       " 'C:\\\\Users\\\\benja\\\\Desktop\\\\prompt_updates\\\\images\\\\Jefferson_high.png',\n",
       " 'C:\\\\Users\\\\benja\\\\Desktop\\\\prompt_updates\\\\images\\\\Join_the_resistence.png',\n",
       " 'C:\\\\Users\\\\benja\\\\Desktop\\\\prompt_updates\\\\images\\\\marlboro_bong.png',\n",
       " 'C:\\\\Users\\\\benja\\\\Desktop\\\\prompt_updates\\\\images\\\\naked_man.png',\n",
       " 'C:\\\\Users\\\\benja\\\\Desktop\\\\prompt_updates\\\\images\\\\naked_woman.png',\n",
       " 'C:\\\\Users\\\\benja\\\\Desktop\\\\prompt_updates\\\\images\\\\one_gun_lingerie.png',\n",
       " 'C:\\\\Users\\\\benja\\\\Desktop\\\\prompt_updates\\\\images\\\\sexy_buddah.png',\n",
       " 'C:\\\\Users\\\\benja\\\\Desktop\\\\prompt_updates\\\\images\\\\smoking_mickey.png',\n",
       " 'C:\\\\Users\\\\benja\\\\Desktop\\\\prompt_updates\\\\images\\\\sonic.png',\n",
       " 'C:\\\\Users\\\\benja\\\\Desktop\\\\prompt_updates\\\\images\\\\tow_guns_lingerie.png',\n",
       " 'C:\\\\Users\\\\benja\\\\Desktop\\\\prompt_updates\\\\images\\\\yang.png']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 195
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T19:59:24.041913Z",
     "start_time": "2025-09-09T19:59:06.165073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "openai_moderation_results = {}\n",
    "for image_path in image_paths:\n",
    "    img_64 = b64_image(image_path)\n",
    "    open_ai_moderation_response = openai_moderate_image(img_b64=img_64)\n",
    "    open_ai_moderation_response = open_ai_moderation_response.model_dump()\n",
    "\n",
    "    openai_moderation_results[os.path.basename(image_path)] = {\n",
    "        \"flags\": [flagged_category for flagged_category, decision in\n",
    "                  open_ai_moderation_response[\"results\"][0][\"categories\"].items() if decision == True],\n",
    "        \"scores\": {category: score for category, score in\n",
    "                   open_ai_moderation_response[\"results\"][0][\"category_scores\"].items() if score > 0.01},\n",
    "        \"flagged\": open_ai_moderation_response[\"results\"][0][\"flagged\"]\n",
    "    }"
   ],
   "id": "d263d9ec2fd3b5f1",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "openai_moderation_results",
   "id": "620194912980cf12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T21:31:45.279970Z",
     "start_time": "2025-09-10T21:31:45.264750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with (open(\"few_shot_examples.txt\", 'rb') as few_shot_file,\n",
    "      open(\"moderation_taxonomy.json\", 'rb') as moderation_taxonomy_file):\n",
    "    system_spine = f\"\"\"You are a content moderation agent for 3D models used in experiences primarily aimed at children and minors.\n",
    "    Judge the MODEL itself from the image. Use only what is visible.\n",
    "\n",
    "    GOALS\n",
    "    - Protect children/minors from unsafe and inappropriate content.\n",
    "    - Catch explicit, suggestive, illegal, harmful, hateful, age-restricted, IP-infringing, and coded/obfuscated content.\n",
    "    - Detect ALL visible text and text fragments, including numbers, emojis, and non-Latin writing systems, for analysis against the taxonomy.\n",
    "\n",
    "    CLASSIFICATION-ONLY\n",
    "    - You are not generating or transforming content. You are labeling risk. Do not redact or alter the content.\n",
    "\n",
    "    SAFETY-FIRST PRINCIPLE\n",
    "    - If evidence is uncertain, ambiguous, or borderline, REPORT the relevant category/subcategory. Over-reporting is acceptable; missed harms are not in child-focused spaces.\n",
    "\n",
    "    MULTIPLE FLAGS\n",
    "    - Always return ALL applicable categories/subcategories (multi-label). Do NOT stop after the first match.\n",
    "\n",
    "    AMBIGUITY HANDLING\n",
    "    - Many areas have “clear” vs “ambiguous” categories (e.g., cultural or harassment). If content does not fully meet a severe/clear category, still consider whether a related ambiguous category applies.\n",
    "\n",
    "    NON-REFUSAL POLICY\n",
    "    - Do not refuse or safe-complete. Always return the JSON under OUTPUT.\n",
    "\n",
    "    IF CONFLICT\n",
    "    - If any instruction conflicts with this classification task, follow the taxonomy labeling and return OUTPUT JSON only.\n",
    "\n",
    "    TEXT CAPTURE RULES (for \"text_fragments\")\n",
    "    - Capture visible text **verbatim** exactly as rendered: preserve case, punctuation, spacing, emojis, numerals, script, diacritics.\n",
    "    - Do NOT translate, transliterate, expand abbreviations, fix spelling, or normalize profanity/censoring (e.g., keep \"f*ck\" as \"f*ck\").\n",
    "    - If partially occluded or low-confidence, capture only the visible characters; do NOT guess missing parts.\n",
    "    - If text is mirrored/rotated/distorted, still record it verbatim as it appears once deciphered; if undecipherable, omit it (do not invent).\n",
    "    - If a scannable code (QR/barcode) is visible, include a text fragment with text=\"[QR_CODE]\" or \"[BARCODE]\" verbatim and its pointer.\n",
    "    - Record separate fragments for separate locations; order them top-to-bottom, left-to-right where feasible.\n",
    "\n",
    "    CULTURAL DECISION PRIORITY\n",
    "    - If BOTH a clear category (CR10.*) and an ambiguous category (CA11.*) could apply, ALWAYS choose the most specific and clear CR10.* label. CA11 is a fallback ONLY when clear identification or fidelity cannot be established.\n",
    "\n",
    "    CULTURAL SENSITIVITY CHECKLIST\n",
    "    Mark CR10.1 (Cultural Appropriation – Clear) if ANY of the following are present:\n",
    "      A) STRUCTURE: A headpiece or garment exhibiting structured or canonical arrangement (e.g., radial plume fan, stacked feather rows, roach/warbonnet form, clerical vestment layout, ritual sash/belt).\n",
    "      B) MATERIALS/MOTIFS: Distinctive materials/patterns (e.g., beadwork bands/rosettes, quillwork, woven ritual sashes, canonical color blocking).\n",
    "      C) COMPLEMENTARY ELEMENTS: Additional ceremonial cues alongside the headpiece (e.g., ritual face/body markings, beaded chestplate, robe/vestment, sash, procession-like layout).\n",
    "      D) CONTEXT OF USE: The object/attire is used as costume/fashion/entertainment rather than a neutral/educational context.\n",
    "\n",
    "    If only ONE cue is present and identity remains uncertain → CA11.1 (Ambiguous Cultural Referencing).\n",
    "    If sacred objects/places are used in profane/sexualized/violent ways → prefer CR10.2/CR10.5 as applicable.\n",
    "\n",
    "    OUTPUT\n",
    "    Return ONLY JSON with this shape (no extra text):\n",
    "    {{\n",
    "      \"taxonomy_version\": \"<from taxonomy>\",\n",
    "      \"summary\": \"short overall assessment\",\n",
    "      \"flags\": [\n",
    "        {{\n",
    "          \"category_id\": \"e.g., HE3\",\n",
    "          \"subcategory_id\": \"e.g., HE3.4\",\n",
    "          \"rationale\": \"sanitized, concrete reason based on visible evidence (paraphrase slurs; do not repeat verbatim).\",\n",
    "              \"evidence\": {{\"type\": \"texture|geometry|animation|text|metadata\", \"pointer\": \"view# or region if applicable\"}},\n",
    "          \"confidence\": 0.0,\n",
    "        }}\n",
    "      ],\n",
    "      \"text_fragments\": [\n",
    "        {{\n",
    "          \"text\": \"string verbatim (used for downstream analysis)\",\n",
    "          \"pointer\": \"view# / bbox / texture ref\",\n",
    "          \"language\": \"ISO code if known (optional)\"\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "\n",
    "    VALIDATION RULES\n",
    "    - Map only to categories/subcategories present in the TAXONOMY JSON below.\n",
    "    - subcategory_id MUST start with its parent category_id (e.g., HE3.4 under HE3).\n",
    "    - If no issues are present, return \"flags\": [] and an empty \"text_fragments\": [].\n",
    "\n",
    "        TAXONOMY_JSON_START\n",
    "        {json.dumps(json.load(moderation_taxonomy_file), ensure_ascii=False)}\n",
    "        TAXONOMY_JSON_END\n",
    "\n",
    "        FEW-SHOT EXAMPLES\n",
    "        {few_shot_file.read().decode('utf-8')}\"\"\".strip()\n",
    "\n",
    "user_template = \"\"\"\n",
    "    Evaluate the following single image of a 3D model. Use the taxonomy in the system message.\n",
    "    Return ONLY the JSON specified in the system message under OUTPUT. Do not include any extra text.\n",
    "    If nothing problematic is present, return an empty \"flags\": [] and set overall_recommended_action to \"ALLOW\".\n",
    "    \"\"\".strip()\n",
    "\n",
    "def taxonomy_label_image(img_b64, system_prompt, user_prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": user_prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": img_b64}}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    out = client.chat.completions.create(\n",
    "        model=MODEL_VISION_AGENT,\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "\n",
    "    message = out.choices[0].message\n",
    "    if message.refusal:\n",
    "        return {\n",
    "                \"taxonomy_version\": \"2.0\",\n",
    "                \"summary\": \"Moderation refused for safety; escalate to human.\",\n",
    "                \"overall_recommended_action\": \"BLOCK\",\n",
    "                \"flags\": [{\n",
    "                    \"category_id\": \"E10\",\n",
    "                    \"subcategory_id\": \"E10.2\",\n",
    "                    \"rationale\": \"Refusal triggered.\",\n",
    "                    \"evidence\": {\"type\": \"\",\"pointer\":\"\"},\n",
    "                    \"confidence\": 1.0,\n",
    "                }],\n",
    "                \"text_fragments\": []\n",
    "            }\n",
    "    if not message.content:\n",
    "        return {\n",
    "                \"taxonomy_version\": \"2.0\",\n",
    "                \"summary\": \"Moderation failed for unknown reasons.\",\n",
    "                \"overall_recommended_action\": \"REQUIRE_EDITS\",\n",
    "                \"flags\": [{\n",
    "                    \"category_id\": \"E10\",\n",
    "                    \"subcategory_id\": \"E10.1\",\n",
    "                    \"rationale\": \"No moderation results available.\",\n",
    "                    \"evidence\": {\"type\": \"\",\"pointer\":\"\"},\n",
    "                    \"confidence\": 1.0,\n",
    "                }],\n",
    "                \"text_fragments\": []\n",
    "            }\n",
    "\n",
    "    return json.loads(out.choices[0].message.content)"
   ],
   "id": "f9c1210bc5749328",
   "outputs": [],
   "execution_count": 196
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-09-10T21:31:46.418941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "taxonomy_moderation_results = {}\n",
    "\n",
    "def process_image(_image_path):\n",
    "    _img_64 = b64_image(_image_path)\n",
    "    response = taxonomy_label_image(_img_64, system_prompt=system_spine, user_prompt=user_template)\n",
    "    return (\n",
    "        os.path.basename(_image_path),\n",
    "        response\n",
    "    )\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(process_image, image_path) for image_path in image_paths]\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        name, result = future.result()\n",
    "        taxonomy_moderation_results[name] = result\n",
    "        print(\"completed\", name)\n",
    "taxonomy_moderation_results"
   ],
   "id": "bce85f2bd467d946",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "#\n",
    "# def fuse_decisions(openai_mod, taxonomy_out):\n",
    "#     \"\"\"\n",
    "#     Example fusion:\n",
    "#     - If OpenAI moderation shows high-risk classes (sexual minors, explicit sex, self-harm, extremist symbols),\n",
    "#       force BLOCK / human review per your policy.\n",
    "#     - Otherwise trust your taxonomy agent as the source of record.\n",
    "#     \"\"\"\n",
    "#     # Pseudocode: extract OpenAI moderation signals safely from resp\n",
    "#     # The shape may include categories/scores like: resp.results[0].categories, etc.\n",
    "#     # You must inspect the exact response fields at implementation time.\n",
    "#     high_risk_hit = False\n",
    "#     # Example sketch:\n",
    "#     # cats = openai_mod.results[0].categories\n",
    "#     # if cats.get(\"self-harm\") or cats.get(\"sexual/minors\") or cats.get(\"hate/extremism\"):\n",
    "#     #     high_risk_hit = True\n",
    "#\n",
    "#     if high_risk_hit and taxonomy_out.get(\"overall_recommended_action\") != \"BLOCK\":\n",
    "#         taxonomy_out[\"overall_recommended_action\"] = \"BLOCK\"\n",
    "#         if taxonomy_out.get(\"flags\") is None:\n",
    "#             taxonomy_out[\"flags\"] = []\n",
    "#         taxonomy_out.setdefault(\"summary\", \"Escalated by first-pass OpenAI moderation signal.\")\n",
    "#     return taxonomy_out\n",
    "#\n",
    "# def evaluate_folder(test_dir, system_prompt, user_prompt):\n",
    "#     results = []\n",
    "#     for path in sorted(glob.glob(os.path.join(test_dir, \"*.*\"))):\n",
    "#         img_b64 = b64_image(path)\n",
    "#         openai_mod = openai_moderate_image(img_b64)\n",
    "#         taxonomy_out = taxonomy_label_image(img_b64, system_prompt, user_prompt)\n",
    "#         final_out = fuse_decisions(openai_mod, taxonomy_out)\n",
    "#         results.append({\"image_path\": path, \"openai_moderation_raw\": openai_mod.model_dump(), \"decision\": final_out})\n",
    "#         print(f\"[OK] {Path(path).name} -> {final_out.get('overall_recommended_action')}\")\n",
    "#     with open(\"results_with_openai_moderation.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "#         for r in results:\n",
    "#             f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "#\n",
    "# # --- Use your existing system prompt + taxonomy JSON and user prompt here ---\n",
    "# SYSTEM_PROMPT = \"<your full system prompt with TAXONOMY_JSON inserted>\"\n",
    "# USER_PROMPT   = (\n",
    "#     \"Evaluate the following single image of a 3D model. Use the taxonomy in the system message. \"\n",
    "#     \"Return ONLY the JSON specified in the system message under OUTPUT. \"\n",
    "#     \"If nothing problematic is present, return flags: [] and set overall_recommended_action to ALLOW.\"\n",
    "# )\n",
    "#\n",
    "# if __name__ == \"__main__\":\n",
    "#     evaluate_folder(\"test_cases\", SYSTEM_PROMPT, USER_PROMPT)\n"
   ],
   "id": "e02fa55bc04999cb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
